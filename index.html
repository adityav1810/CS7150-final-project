<!doctype html>
<html lang="en">

<head>
  <title>Vision Transformers</title>
  <meta property="og:title" content=An Image is worth 16 x 16 words" />
  <meta name="twitter:title" content="An Image is worth 16 x 16 words" />
  <meta name="description" content="Your project about your cool topic described right here." />
  <meta property="og:description" content="Your project about your cool topic described right here." />
  <meta name="twitter:description" content="Your project about your cool topic described right here." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

  <!-- Add some styling -->
  <style>
    table {
      border-collapse: collapse;
      width: 100%;
    }

    th,
    td {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }

    th {
      background-color: #f2f2f2;
    }
  </style>


</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">An Image is worth 16 x 16 words</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>An Analysis of AN IMAGE IS WORTH 16X16 WORDS :</h2>
        <p>Describe the paper and the big question about it that interests you.</p>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <h2>Literature Review</h2>
        <p>The main motivation of behind ViT was the success of using Transformers for NLP tasks. The dominant approach
          of pre-training the model on large dataset and fine-tuning by small task-specific datasets (BERT and GPT) was
          limited to textual data, as directly applying self-attention mechanism to pixels was computationally
          intractable due to very large input size (image pixels). Often attention mechanism was considered in
          conjunction with convolutional layers.</p>
        <p>Some attempts had been made to make the self-attention mechanism work for images:</p>
        <ul>
          <li>Replacing global self-attention with just local, neighboring pixels:
            <ul>
              <li>Image Transformer (Parmar 2018)</li>
              <li>Stand-Alone Self-Attention in Vision Models (Ramachandran 2019)</li>
              <li>Exploring Self-attention for Image Recognition (Zhao 2020)</li>
            </ul>
          </li>
          <li>Approximations to global self-attention:
            <ul>
              <li>Sparse Transformers (Child 2019)</li>
            </ul>
          </li>
          <li>Applying the attention mechanism for just rows/columns:
            <ul>
              <li>Axial attention in multidimensional transformers (Ho 2019)</li>
            </ul>
          </li>
        </ul>
        <p>Many of these specialized attention architectures above demonstrated promising results on computer vision
          tasks, but require complex engineering to be implemented efficiently on hardware accelerators.</p>
        <p>Most significant and similar work to ViT was On the Relationship between Self-Attention and Convolutional
          Layer (Cordonnier 2020). Key finding: “we prove that a multi-head self-attention layer with sufficient number
          of heads is at least as expressive as any convolutional layer”. However, the paper lacked the evaluation of
          the method against state-of-the-art models, was limited to 2x2 image patches
          that hindered the scalability and thus only worked on small resolution images.</p>
        <p>The authors of the paper built on top of their prior work Big Transfer (BiT) on CNN transfer learning
          (Kolesnikov 2020 and Djolonga 2020), comparing the approach already done on ResNet to the Transformer
          architecture.</p>
        <h2>Biography</h2>
        <p>
          This team doesnt have well known names in it, However they are highly motivated researchers working at
          Google interested in applying deep learning techniques to vision related tasks.
          Some members are now at different research groups but have collaborated together frequently in the past.
          Another major research they conducted was "Scaling Vision Transformers to 22 billion parameters".
          Most of the researches in this study work on scaling deep networks for vision tasks. <br>
          <!-- 

Alexander Kolesnikov : Research Engineer @ Google Deepmind. Previously PhD at IST Austria, and applied math MSc at Moscow State University.<br>
Dirk Weissenborn :  Based out of Germany, Ex-Google, Meta, DeepMind. currently working at Inceptive Inc. <br>
Xiaohua Zhai : Seniour Staff Researcher at Google, Zurich. PhD from Peking University, China. Interested in vision, representation learning and generative modelling. <br>
Thomas Unterthiner : ML Engineer at Google, PhD from Johannes Kepler Universitat, Germany.<br>
Mostafa Dehghani : Research Scientist at Google, Ex-Apple. PhD from University of Amsterdam. <br>
Matthias Minderer : Senior Research Scientist at Google, PhD from Harvard under Christopher Harvey, Ex Neuroscience Major at ETH Zurich, also studied Biochemistry at University of Cambridge . Interested in representation learning of vision tasks. <br>
Georg Heigold : Research Scientist at Google, Mountain View , CA. Diploma in Physics from ETH Zurich. His research interests include automatic speech recognition, discriminative training, and log-linear modeling.<br>
Sylvain Gelly : Deep learning researcher at Google Brain, Zurich. Likes to work on reinforcement learning and dynamic programming. <br>
Jakob Uszkoreit : One of the authors of the original transformer paper, Maybe one of the most cited authors on this panel. Cofounded Inceptive  Ex Googler. <br>
Neil Houlsby : Senior Staff Research Scientist at Google, Zurich. PhD  in Computational Biology from University of Cambridge. Research interests include Bayesian ML, Cognitive Science and Active Learning. <br> -->
        <table>
          <tr>
            <th>Name</th>
            <th> Photo </th>
            <th>Affiliation</th>
            <th>Contribution</th>

          </tr>
          <tr>
            <td>Alexey Dosovitskiy</td>
            <td><img src="images/authors/alexd.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Ex Intel, left Google for an year to setup Inceptive with Jakob, recently returned back. PhD in
              Mathematics from Lomonosov Moscow State University. </td>

          </tr>
          <tr>
            <td>Lucas Beyer</td>
            <td><img src="images/authors/lucasb.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Co-author, involved in the exploration and experimentation with Vision Transformers.French- German
              national from Belgium with PhD from RWTH Aachen.</td>
          </tr>
          <tr>
            <td>Alexander Kolesnikov</td>
            <td><img src="images/authors/alexanderK.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Previously PhD at IST Austria, and applied math MSc at Moscow State University.</td>
          </tr>
          <tr>
            <td>Dirk Weissenborn</td>
            <td><img src="images/authors/dirk.jpeg" height="100px" width="100px"></td>
            <td>Inceptive Inc.</td>
            <td>Based out of Germany, Ex-Google, Meta, DeepMind.</td>
          </tr>
          <tr>
            <td>Xiaohua Zhai</td>
            <td><img src="images/authors/xiaohua.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Seniour Staff Researcher based out of Zurich. PhD from Peking University, China. Interested in vision,
              representation learning and generative modelling.</td>
          </tr>
          <tr>
            <td>Thomas Unterthiner</td>
            <td><img src="images/authors/thomas.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>ML Engineer , PhD from Johannes Kepler Universitat, Germany. Research interests include Compuational
              Biology, understanding deep networks, activation functions, emaluation metrics </td>
          </tr>
          <tr>
            <td>Mostafa Dehghani</td>
            <td><img src="images/authors/mostafa.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td> Research Scientist, Ex-Apple. PhD from University of Amsterdam. </td>
          </tr>
          <tr>
            <td>Matthias Minderer</td>
            <td><img src="images/authors/matthias.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>PhD from Harvard under Christopher Harvey, Ex Neuroscience Major at ETH Zurich, also studied
              Biochemistry at University of Cambridge . Interested in representation learning of vision tasks.</td>
          </tr>
          <tr>
            <td>Georg Heigold</td>
            <td><img src="images/authors/georg.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Diploma in Physics from ETH Zurich. His research interests include automatic speech recognition,
              discriminative training, and log-linear modeling. Ex-Apple</td>
          </tr>
          <tr>
            <td>Sylvain Gelly</td>
            <td><img src="images/authors/sylvain.jpeg" height="100px" width="100px"></td>
            <td>Independent Researcher / Google Research</td>
            <td>Deep learning Researcher based out of Zurich. Likes to work on reinforcement learning and dynamic
              programming.</td>
          </tr>
          <tr>
            <td>Jakob Uszkoreit</td>
            <td><img src="images/authors/jakob.jpeg" height="100px" width="100px"></td>
            <td>Co-founder @ Inceptive</td>
            <td>One of the authors of the original transformer paper, Maybe one of the most cited authors on this
              panel. Ex Googler.</td>
          </tr>
          <tr>
            <td>Neil Houlsby</td>
            <td><img src="images/authors/neil.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>PhD in Computational Biology from University of Cambridge. Research interests include Bayesian ML,
              Cognitive Science and Active Learning.</td>
          </tr>

          <!-- Add more rows for other authors -->
        </table>
        </p>

        <h2>
          Novel Work by the Paper (Diagrammer)
        </h2>
        <br>

        <strong>
          The variants of the ViT architecture follow the ones from BERT:
        </strong>
        <div class="text-center">
          <img src="images/figures/tab1.png" width="50%">
        </div>
        <br>

        <strong>
          Comparing to state-of-the-art models on popular image classification:
        </strong>
        <div class="text-center">
          <img src="images/figures/tab2.png" width="60%">
        </div>
        <br>

        <strong>
          Pre-training data and computation requirements compared to BiT:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig3-4.png" width="60%">
          <img src="images/figures/fig5.png" width="60%">
        </div>
        <br>

        <strong>
          Some lower layer attention heads behave like convolutional layers, focusing on local patches, while others
          have a more global view:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig7.png" width="60%">
        </div>
        <p>
          The authors have experimented with different positional embeddings, curiously, explicitly including 2-D
          information in them does not improve performance, as the model learns to encode it implicitly:
        </p>
        <div class="text-center">
          <img src="images/figures/tab8.png" width="60%">
        </div>
        <br>

        <strong>
          Using Attention Rollout to visualize the attention maps show that the model learns to attend to the
          semantically relevant parts of the image:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig6.png" width="20%">
        </div>


        <h2>Social Impact</h2>
        <p>
          Independent of paper, personal research by Gega.


        </p>
        <h2>Industry Applications</h2>
        <p>
          Independent of paper, personal research by Aditya.
        </p>
        <h2>Follow-on Research</h2>
        <p>
          Independent of paper, personal research by Aditya.
        </p>
        <h2>Peer Review</h2>
        <p>
          Independent of paper, personal research by Aditya.
        </p>

        <p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report
          on your findings.
        </p>

        <h3>References</h3>

        <p><a name="vision-transformer">[1]</a> <a href="https://arxiv.org/pdf/2010.11929.pdf">L&eacute;on Bottou and
            Patrick Gallinari.
            <em>A framework for the cooperation of learning algorithms.</em></a>
          Advances in neural information processing systems 3 (1990).
        </p>

        <h2>Team Members</h2>

        <p>
        <ul>
          <li>Aditya Varshney</li>
          <li> Gega Darakhvelidze </li>
        </ul>

        </p>


      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>