<!doctype html>
<html lang="en">

<head>
  <title>Vision Transformers</title>
  <meta property="og:title" content=An Image is worth 16 x 16 words" />
  <meta name="twitter:title" content="An Image is worth 16 x 16 words" />
  <meta name="description" content="Your project about your cool topic described right here." />
  <meta property="og:description" content="Your project about your cool topic described right here." />
  <meta name="twitter:description" content="Your project about your cool topic described right here." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">

  <!-- Add some styling -->
  <style>
    table {
      border-collapse: collapse;
      width: 100%;
    }

    th,
    td {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }

    th {
      background-color: #f2f2f2;
    }


    .center {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
    }
  </style>


</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">An Image is worth 16 x 16 words</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>An Analysis of AN IMAGE IS WORTH 16X16 WORDS :</h2>
        <p>Describe the paper and the big question about it that interests you.</p>
        <figure class="center">
          <img src="./images/figures/transformer-architecture.gif" width="80%">
          <figcaption>Figure1: source Google AI blog</figcaption>
        </figure>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <h2>Literature Review</h2>

        <p>The main motivation of behind ViT was the success of using Transformers for NLP tasks. The dominant approach
          of pre-training the model on large dataset and fine-tuning by small task-specific datasets (BERT and GPT) was
          limited to textual data, as directly applying self-attention mechanism to pixels was computationally
          intractable due to very large input size (image pixels). Often attention mechanism was considered in
          conjunction with convolutional layers.</p>
        <p>Some attempts had been made to make the self-attention mechanism work for images:</p>
        <ul>
          <li>Replacing global self-attention with just local, neighboring pixels:
            <ul>
              <li>Image Transformer (Parmar 2018)</li>
              <li>Stand-Alone Self-Attention in Vision Models (Ramachandran 2019)</li>
              <li>Exploring Self-attention for Image Recognition (Zhao 2020)</li>
            </ul>
          </li>
          <li>Approximations to global self-attention:
            <ul>
              <li>Sparse Transformers (Child 2019)</li>
            </ul>
          </li>
          <li>Applying the attention mechanism for just rows/columns:
            <ul>
              <li>Axial attention in multidimensional transformers (Ho 2019)</li>
            </ul>
          </li>
        </ul>
        <p>Many of these specialized attention architectures above demonstrated promising results on computer vision
          tasks, but require complex engineering to be implemented efficiently on hardware accelerators.</p>
        <p>Most significant and similar work to ViT was On the Relationship between Self-Attention and Convolutional
          Layer (Cordonnier 2020). Key finding: "we prove that a multi-head self-attention layer with sufficient number
          of heads is at least as expressive as any convolutional layer". However, the paper lacked the evaluation of
          the method against state-of-the-art models, was limited to 2x2 image patches
          that hindered the scalability and thus only worked on small resolution images.</p>
        <p>The authors of the paper built on top of their prior work Big Transfer (BiT) on CNN transfer learning
          (Kolesnikov 2020 and Djolonga 2020), comparing the approach already done on ResNet to the Transformer
          architecture.</p>
        <h2>Biography</h2>
        <p>
          This team doesnt have well known names in it, However they are highly motivated researchers working at
          Google interested in applying deep learning techniques to vision related tasks.
          Some members are now at different research groups but have collaborated together frequently in the past.
          Another major research they conducted was "Scaling Vision Transformers to 22 billion parameters".
          Most of the researches in this study work on scaling deep networks for vision tasks. <br>

        <table>
          <tr>
            <th>Name</th>
            <th> Photo </th>
            <th>Affiliation</th>
            <th>Contribution</th>

          </tr>
          <tr>
            <td>Alexey Dosovitskiy</td>
            <td><img src="images/authors/alexd.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Ex Intel, left Google for an year to setup Inceptive with Jakob, recently returned back. PhD in
              Mathematics from Lomonosov Moscow State University. </td>

          </tr>
          <tr>
            <td>Lucas Beyer</td>
            <td><img src="images/authors/lucasb.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Co-author, involved in the exploration and experimentation with Vision Transformers.French- German
              national from Belgium with PhD from RWTH Aachen.</td>
          </tr>
          <tr>
            <td>Alexander Kolesnikov</td>
            <td><img src="images/authors/alexanderK.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Previously PhD at IST Austria, and applied math MSc at Moscow State University.</td>
          </tr>
          <tr>
            <td>Dirk Weissenborn</td>
            <td><img src="images/authors/dirk.jpeg" height="100px" width="100px"></td>
            <td>Inceptive Inc.</td>
            <td>Based out of Germany, Ex-Google, Meta, DeepMind.</td>
          </tr>
          <tr>
            <td>Xiaohua Zhai</td>
            <td><img src="images/authors/xiaohua.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Seniour Staff Researcher based out of Zurich. PhD from Peking University, China. Interested in vision,
              representation learning and generative modelling.</td>
          </tr>
          <tr>
            <td>Thomas Unterthiner</td>
            <td><img src="images/authors/thomas.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>ML Engineer , PhD from Johannes Kepler Universitat, Germany. Research interests include Compuational
              Biology, understanding deep networks, activation functions, emaluation metrics </td>
          </tr>
          <tr>
            <td>Mostafa Dehghani</td>
            <td><img src="images/authors/mostafa.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td> Research Scientist, Ex-Apple. PhD from University of Amsterdam. </td>
          </tr>
          <tr>
            <td>Matthias Minderer</td>
            <td><img src="images/authors/matthias.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>PhD from Harvard under Christopher Harvey, Ex Neuroscience Major at ETH Zurich, also studied
              Biochemistry at University of Cambridge . Interested in representation learning of vision tasks.</td>
          </tr>
          <tr>
            <td>Georg Heigold</td>
            <td><img src="images/authors/georg.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>Diploma in Physics from ETH Zurich. His research interests include automatic speech recognition,
              discriminative training, and log-linear modeling. Ex-Apple</td>
          </tr>
          <tr>
            <td>Sylvain Gelly</td>
            <td><img src="images/authors/sylvain.jpeg" height="100px" width="100px"></td>
            <td>Independent Researcher / Google Research</td>
            <td>Deep learning Researcher based out of Zurich. Likes to work on reinforcement learning and dynamic
              programming.</td>
          </tr>
          <tr>
            <td>Jakob Uszkoreit</td>
            <td><img src="images/authors/jakob.jpeg" height="100px" width="100px"></td>
            <td>Co-founder @ Inceptive</td>

            <td>One of the authors of the original transformer paper, Maybe one of the most cited authors on this
              panel. Ex Googler.</td>
          </tr>
          <tr>
            <td>Neil Houlsby</td>
            <td><img src="images/authors/neil.jpeg" height="100px" width="100px"></td>
            <td>Google Research</td>
            <td>PhD in Computational Biology from University of Cambridge. Research interests include Bayesian ML,
              Cognitive Science and Active Learning.</td>
          </tr>

          <!-- Add more rows for other authors -->
        </table>
        </p>

        <h2>
          Novel Work by the Paper (Diagrammer)
        </h2>

        <p>
          Transformers had been recently proposed by Vaswani et al in <a href="#transformer">[2]</a> and had become
          state of the art for language related tasks. The authors of this paper wanted to come up with a solution
          which was as close as possible to the original Transformer so that other language based approaches to scale
          the architecture could be used out of the box.
          Hence, they chose to use the same architecture setup as done by the authors in <a href="#transformer">[2]</a>.
          However, as the original Transformer requires a 1D sequence of tokens, the authors had to figure out a way to
          convert an image, which is 2-Dimensional, into a 1-D sequence of information.<br>

          In order to solve this, the input images (of dimension <i>H</i> and <i>W</i>) were reshaped into <i>N</i>
          patches of size <i>P</i>,
          where in <i>N</i> was calculated by computing HW &frasl; P<sup>2</sup>. The output of this operation were
          described as patch embeddings.
          Similary to its natural language counterpart which uses BERT's [class] tokens, it was essential to prepend a
          learnable embedding to
          the patch embeddings whose state at the output of the Encoder block could be used as the image
          representations. <br>

          The transformer architecture is a suitable architecture for language related tasks as they are rich in
          sequence and the architecture adds encodes the position of text
          along with the text as position embeddings. These positional embeddings are key to the architecture. In order
          to mimic this charachterstic in vision, positional embeddings
          are added into the patch embeddings which help retain position information of the patches. The resulting
          embedding vector is used as input to the encoder block of the transformer.


        <h4> The Vision Transformer architecture</h4>

        <p>
          Up until this time, CNNs based architectures achieved state-of-the-art performance of vision tasks due to
          their wonderful ability to have robust neighbourhood structure
          and translational equivaraince, however, as the network grew deeper they were prone to information loss.
          Residual Connections mitigated this to some extent.
          However, the transformer block consists of MLP units which capture local and translational equivaraince and
          the Attention units are focused on capturing global context.
          This ability to inherintly capture local as well as global context make the transformer architecture solve

          The original architecture consists of alternating layers of multihead self-attention (MSA) and MLP blocks with
          LayerNorm and Residual Connections being applied after every block. Mathematically, the layers can be
          described as shown in equation <a href="#eq1">1</a> <br>
        </p>
        <figure id="eq1" class="text-center">
          <img src="./images/figures/eq-1.jpg" width="60%">
          <figcaption>Equation 1 : Mathematical expression for transformer layers </figcaption>
        </figure>
        <p>
          As the transformer block does not have any CNN layers which may lead to neighbourhood structure being learnt,
          the model instead learns the 2D positions as well as the spatial relations
          of the patches making it free of any sort of inductive bias.
          Furthermore, the authors propose another way to generate patchh embeddings by using a CNN feature map to
          extract
          patches instead of generating raw patches.
        </p>
        <br>

        <strong>
          The variants of the ViT architecture follow the ones from BERT:
        </strong>
        <div class="text-center">
          <img src="images/figures/tab1.png" width="50%">
        </div>
        <br>

        <strong>
          Comparing to state-of-the-art models on popular image classification:
        </strong>
        <div class="text-center">
          <img src="images/figures/tab2.png" width="60%">
        </div>
        <br>

        <strong>
          Pre-training data and computation requirements compared to BiT:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig3-4.png" width="60%">
          <img src="images/figures/fig5.png" width="60%">
        </div>
        <br>

        <strong>
          Some lower layer attention heads behave like convolutional layers, focusing on local patches, while others
          have a more global view:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig7.png" width="60%">
        </div>
        <p>
          The authors have experimented with different positional embeddings, curiously, explicitly including 2-D
          information in them does not improve performance, as the model learns to encode it implicitly:
        </p>
        <div class="text-center">
          <img src="images/figures/tab8.png" width="60%">
        </div>
        <br>

        <strong>
          Using Attention Rollout to visualize the attention maps show that the model learns to attend to the
          semantically relevant parts of the image:
        </strong>
        <div class="text-center">
          <img src="images/figures/fig6.png" width="20%">
        </div>


        <h2>Social Impact</h2>
        <p>
          Independent of paper, personal research by Gega.


        </p>
        <h2>Industry Applications</h2>
        <p>

          Due to the rich latent space that vision transformer learn, this architecture was quickly adopted in the
          industry
          and has become one of the most used architectures for production grade ML models. Some of the industries which
          use Vision based transformers are :

        <ul>
          <li> Autonomous Vehicle: It is highly speculated that Tesla's <a
              href="https://www.youtube.com/watch?v=j0z4FweCy4M">Full Self-Driving software</a>
            uses a Vision transformer based architecture as its backbone. Similary Nvidia's self driving car platform as
            well is rumored to have a transformer based backbone!

          </li>
          <li>
            Medical Imaging
          </li>
          <li>
            Augumented Reality
          </li>

        </ul>
        </p>
        <h2>Follow-on Research</h2>
        <p>
          This study was the first to bring the transformer architecture to vision which soon became one of the most
          popular methods.
          This led to various aspects of the model being subjected to improvement. A few areas of research which have
          led to significant improvements in the transformer architecture are
        <ul>
          <li>
            The authors of this paper are really interested in scaling the models, they leverage the power laws and
            propose structural changes to the
            architecture which results in the following research <a href="https://arxiv.org/abs/2106.04560">Scaling
              Vision Transformers</a><br>
          </li>


          <li> Since then many studies have tried to make the architecture more efficient, some of which are
            <ul>
              <li>
                <a href="https://arxiv.org/abs/2107.01378">Efficient Vision Transformers via Fine-Grained Manifold
                  Distillation</a>
              </li>
              <li> <a href="https://arxiv.org/abs/2106.09785"> EsViT (Efficient self-supervised Vision Transformers)</a>
              </li>

            </ul>
          </li>

          <li>
            Many attempts have been made to introduce add convolutions to the transformer blocks in order
            to make the model more efficient and robust. Some significant approaches are:
            <ul>
              <li><a href="https://arxiv.org/abs/2103.15808">CvT (Introducing convolutions to Vision Transformers)</a>
              </li>
              <li><a href="https://arxiv.org/abs/2103.10697">ConViT (Improving Vision Transformers with Soft
                  Convolutional Inductive Biases)</a> </li>
              <li><a href="https://arxiv.org/abs/2104.06399">Co-Scale Conv-Attentional Image Transformers (CoaT)</a>
              </li>
              <li><a href="https://arxiv.org/abs/2104.12533">Visformer (The Vision-friendly Transformer)</a> </li>

            </ul>
          </li>
          <li>
            Liu et al propose the <a href=" https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer </a>which leverages a
            heirarchical transformer whose representation is
            computed with shifted windows which limits self-attention to non-overlapping local windows and instead uses
            cross attention for global context.
          </li>

        </ul>
        </p>
        <h2>Peer Review</h2>
        <p>
          Independent of paper, personal research by Aditya.
        </p>


        <p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on
          your findings.

        </p>

        <h3>References</h3>


        <p>
          <a name="vision-transformer">[1]</a> <a href="https://arxiv.org/pdf/2010.11929.pdf">
            Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale."
            arXiv preprint arXiv:2010.11929 (2020).</a><br>
          <a name="transformer">[2]</a><a
            href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">
            Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30
            (2017).</a><br>


        </p>

        <h2>Team Members</h2>

        <p>
        <ul>
          <li>Aditya Varshney</li>
          <li> Gega Darakhvelidze </li>
        </ul>

        </p>


      </div><!--col-->
    </div><!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>